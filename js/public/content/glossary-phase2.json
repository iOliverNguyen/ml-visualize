{
  "parameter-space": {
    "term": "Parameter Space",
    "brief": "The multi-dimensional space where each axis represents one parameter. Points in this space are parameter configurations.",
    "detailed": "Parameter space (or weight space) is the mathematical space formed by treating each parameter as a dimension. For w1 and w2, it's a 2D plane where any point (w1, w2) represents a specific model configuration. Training traces a trajectory through this space from initialization to optimum.",
    "comprehensive": "Parameter space is the geometric representation of all possible parameter values.\n\nFor Phase 2 (2 parameters):\n- Space is 2D: axes are w1 and w2\n- Each point (w1, w2) represents one model\n- Training = path through this space\n- Gradient = direction to move at current position\n\nFor neural networks:\n- Space has millions of dimensions (one per parameter)\n- Impossible to visualize directly\n- But the math is identical: gradient descent follows steepest descent in high-dimensional parameter space\n\nKey concepts:\n- **Trajectory**: The sequence of parameter values visited during training\n- **Starting point**: Initial parameters (usually w1=0, w2=0)\n- **Optimum**: Point where loss is minimized (gradient = 0)\n- **Distance**: Euclidean distance: sqrt((w1-w1_init)² + (w2-w2_init)²)\n\nVisualization:\nThe ParameterSpace2D plot shows this space directly. The x-axis is w1, y-axis is w2. Your training trajectory is the path from start (green) to finish (red).",
    "level": ["beginner", "intermediate", "advanced"],
    "formula": "\\text{Point in space: } (w_1, w_2) \\in \\mathbb{R}^2",
    "relatedTerms": ["trajectory", "gradient-vector", "loss-surface"],
    "example": "At step 0, you're at (0, 0) in parameter space. After 50 steps with lr-optimal, you're at (1.97, 1.52), close to the optimum at (2.0, 1.5). The ParameterSpace2D plot shows this as a line from origin to near (2, 1.5)."
  },
  "loss-surface": {
    "term": "Loss Surface",
    "brief": "The 3D surface showing loss values L(w1, w2) for every possible parameter combination.",
    "detailed": "The loss surface represents the loss function as a 3D landscape where height represents loss value. For 2 parameters, we can visualize it as terrain: valleys are low loss (good), peaks are high loss (bad). The contour plot projects this surface onto 2D by showing level curves. Gradient descent navigates this surface by always moving downhill.",
    "comprehensive": "The loss surface is the complete visualization of how loss depends on parameters.\n\nMathematical definition:\nL: ℝ² → ℝ\nL(w1, w2) = (1/n) Σ (w1·x1 + w2·x2 - y)²\n\nKey properties for linear regression:\n1. **Convex**: Bowl-shaped, single global minimum\n2. **Smooth**: Differentiable everywhere\n3. **Quadratic**: Loss grows quadratically with distance from optimum\n\nSurface shapes:\n- **Circular contours** (isotropic): Loss changes equally in all directions, easy optimization\n- **Elliptical contours** (anisotropic): Loss changes faster in some directions, harder optimization\n- **Narrow valleys**: Elongated regions requiring careful learning rate tuning\n\nVisualization approaches:\n1. **Heatmap**: Color-coded 2D projection (blue=low, red=high)\n2. **Contour lines**: Level curves connecting equal-loss points\n3. **3D surface**: True 3D visualization (not shown in this demo)\n\nThe LossContour visualization shows this surface via heatmap with trajectory overlay. You can see why optimization follows its specific path - it's always descending the surface.",
    "level": ["beginner", "intermediate", "advanced"],
    "formula": "L(w_1, w_2) = \\frac{1}{n} \\sum_{i=1}^{n} (w_1 x_{1i} + w_2 x_{2i} - y_i)^2",
    "relatedTerms": ["contour-plot", "gradient-vector", "anisotropy", "heatmap"],
    "example": "In lr-optimal: loss surface has circular contours centered at (2.0, 1.5) where loss ≈ 0. At origin (0, 0), loss = 89.4. At (1.0, 1.0), loss ≈ 10. The surface is a smooth bowl guiding optimization toward the center."
  },
  "contour-plot": {
    "term": "Contour Plot",
    "brief": "Lines connecting points in parameter space with equal loss values, like elevation contours on a topographic map.",
    "detailed": "Contour lines (also called level sets) trace out all (w1, w2) combinations that yield the same loss value. They reveal the loss surface shape: circular contours indicate isotropic (balanced) surfaces, while elliptical contours indicate anisotropic (imbalanced) surfaces. Gradient vectors are always perpendicular to contour lines.",
    "comprehensive": "Loss contours visualize the loss function L(w1, w2) as a 2D map.\n\nMathematical definition:\nA contour at loss value c is the set:\n{(w1, w2) : L(w1, w2) = c}\n\nKey insights from contour shapes:\n\n1. **Circular contours** (lr-optimal):\n   - Loss changes equally in all directions\n   - Called isotropic or well-conditioned\n   - Gradient descent moves directly toward center\n   - Any learning rate in reasonable range works\n   - Occurs when features have similar scales\n\n2. **Elliptical contours** (anisotropic-hard):\n   - Loss changes faster in some directions\n   - Called anisotropic or ill-conditioned  \n   - Gradient descent zigzags across narrow valley\n   - Very sensitive to learning rate choice\n   - Caused by feature scale mismatches\n\n3. **Contour spacing**:\n   - Close together: steep region (large gradient)\n   - Far apart: flat region (small gradient)\n   - Center: optimum (loss minimum)\n\nRelationship to gradient:\n- Gradient ∇L is perpendicular to contours\n- Points toward higher loss contours\n- Magnitude relates to contour spacing\n\nVisualization:\nThe LossContour heatmap shows continuous loss values (color) with optional contour line overlays. Blue = low loss (goal), red = high loss (avoid). Your trajectory should move from outer contours toward inner contours.",
    "level": ["beginner", "intermediate", "advanced"],
    "formula": "\\{(w_1, w_2) : L(w_1, w_2) = c\\} \\text{ for constant } c",
    "relatedTerms": ["loss-surface", "anisotropy", "gradient-vector", "isotropic-surface"],
    "example": "In lr-optimal, contours are nearly circular around (2.0, 1.5). Loss = 89 at (0,0), loss = 10 at (1.0, 1.0), loss = 2 at (1.9, 1.4), loss ≈ 0 at (2.0, 1.5). Trajectory crosses contours perpendicularly, moving from outer (high loss) to center (low loss)."
  },
  "gradient-vector": {
    "term": "Gradient Vector",
    "brief": "A vector containing partial derivatives of loss with respect to each parameter: [∂L/∂w1, ∂L/∂w2].",
    "detailed": "The gradient vector (denoted ∇L) points in the direction of steepest increase in loss. It has one component per parameter, each showing how loss changes if that parameter increases slightly. Magnitude ||∇L|| indicates how steep the loss surface is. We move opposite to the gradient (descent direction) to minimize loss.",
    "comprehensive": "The gradient vector is the multi-dimensional generalization of the derivative.\n\nDefinition:\n∇L = [∂L/∂w1, ∂L/∂w2]\n\nwhere:\n∂L/∂w1 = (2/n) Σ (w1*x1 + w2*x2 - y_true) * x1\n∂L/∂w2 = (2/n) Σ (w1*x1 + w2*x2 - y_true) * x2\n\nProperties:\n\n1. **Direction**: Points toward increasing loss (uphill)\n   - Gradient descent moves opposite: -∇L (downhill)\n   - This is the steepest descent direction\n\n2. **Magnitude**: ||∇L|| = sqrt(grad_w1² + grad_w2²)\n   - Large magnitude: steep slope, far from minimum\n   - Small magnitude: gentle slope, near minimum\n   - Zero magnitude: at critical point (hopefully minimum!)\n\n3. **Components**: Each partial derivative acts independently\n   - grad_w1: how loss changes with w1 (holding w2 fixed)\n   - grad_w2: how loss changes with w2 (holding w1 fixed)\n   - Update: w_i ← w_i - lr * grad_w_i for each i\n\n4. **Geometric interpretation**:\n   - Perpendicular to contour lines on loss surface\n   - Points directly up the loss surface slope\n   - Length proportional to slope steepness\n\nVisualization:\n- GradientVectorViz: Shows current gradient as large blue arrow\n- GradientField: Shows gradient vectors at many positions\n- Contour lines: Gradient is always perpendicular to these",
    "level": ["beginner", "intermediate", "advanced"],
    "formula": "\\nabla L = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_1} \\\\ \\frac{\\partial L}{\\partial w_2} \\end{bmatrix}, \\quad ||\\nabla L|| = \\sqrt{(\\frac{\\partial L}{\\partial w_1})^2 + (\\frac{\\partial L}{\\partial w_2})^2}",
    "relatedTerms": ["parameter-space", "update-vector", "gradient-magnitude", "gradient-direction"],
    "example": "In lr-optimal at step 0: ∇L = [-52.0, -49.9]. Magnitude: 72.1 (steep!). Direction: points toward lower-left. Update moves opposite: [+0.52, +0.50] scaled by lr=0.01. By step 50: ∇L = [-0.8, -0.6], magnitude 1.0 (much gentler)."
  },
  "trajectory": {
    "term": "Trajectory",
    "brief": "The sequence of parameter values visited during training, forming a path through parameter space.",
    "detailed": "The trajectory is the ordered list of all (w1, w2) values from training start to finish: [(w1_0, w2_0), (w1_1, w2_1), ..., (w1_T, w2_T)]. Visualized as a line path in the ParameterSpace2D plot, it reveals optimization behavior: smooth convergence shows efficient training, while zigzag patterns indicate overshooting problems.",
    "comprehensive": "The trajectory captures the complete history of parameter evolution during training.\n\nRepresentation:\n- Discrete: List of (w1_t, w2_t) for t = 0, 1, 2, ..., T\n- Continuous: Interpolated path connecting these points\n- Visualization: Line plot in ParameterSpace2D\n\nCharacteristics reveal training quality:\n\n1. **Smooth, direct path** (lr-optimal):\n   - Efficient optimization\n   - Learning rate well-tuned\n   - Monotonic progress toward optimum\n   - Path roughly follows -∇L direction\n\n2. **Zigzag pattern** (lr-large, anisotropic-hard):\n   - Overshooting problem\n   - Learning rate too large or anisotropic surface\n   - Bouncing across valley\n   - Inefficient but eventually converges\n\n3. **Tiny movements** (lr-small):\n   - Learning rate too small\n   - Would take forever to converge\n   - Waste of computation time\n\n4. **Curved path** (saddle-point):\n   - Gradient direction changes substantially\n   - Normal behavior when far from optimum\n   - Straightens as approaches bowl-shaped region\n\nAnalyzing trajectories:\n- **Path length**: Total distance traveled Σ||w_t - w_{t-1}||\n- **Displacement**: Direct distance ||w_final - w_init||\n- **Efficiency**: Displacement / Path length (1.0 is perfect straight line)\n- **Convergence**: Distance to optimum ||w_t - w*|| over time",
    "level": ["beginner", "intermediate", "advanced"],
    "formula": "\\mathcal{T} = \\{(w_1^{(t)}, w_2^{(t)})\\}_{t=0}^T",
    "relatedTerms": ["parameter-space", "gradient-vector", "zigzag-pattern"],
    "example": "In lr-optimal: trajectory starts at (0, 0), moves smoothly through (0.52, 0.50) → (0.98, 0.94) → (1.38, 1.31) → ... → (1.97, 1.52), ending near optimum (2.0, 1.5). Total path length: ~2.8 units. Direct distance: ~2.5 units. Efficiency: 89%."
  },
  "gradient-magnitude": {
    "term": "Gradient Magnitude",
    "brief": "The length of the gradient vector: ||∇L|| = √(grad_w1² + grad_w2²). Indicates steepness of loss surface.",
    "detailed": "Gradient magnitude measures how steep the loss surface is at your current position. Large values (like 72 at start) mean you're far from the minimum on a steep slope. Small values (like 0.5 near convergence) mean you're on gentle terrain near the optimum. Zero magnitude means you're at a critical point where gradient descent stops.",
    "comprehensive": "Gradient magnitude is a scalar summary of the vector gradient.\n\nDefinition:\n||∇L|| = sqrt((∂L/∂w1)² + (∂L/∂w2)²)\n\nThis is the Euclidean norm (L2 norm) of the gradient vector.\n\nInterpretation:\n1. **Large magnitude** (||∇L|| > 10):\n   - Far from minimum\n   - Loss changing rapidly\n   - Large parameter updates\n   - Common at training start\n\n2. **Medium magnitude** (1 < ||∇L|| < 10):\n   - Moderate distance from minimum\n   - Steady progress\n   - Typical mid-training\n\n3. **Small magnitude** (||∇L|| < 1):\n   - Near minimum\n   - Loss nearly flat\n   - Tiny updates\n   - Approaching convergence\n\n4. **Near zero** (||∇L|| < 0.01):\n   - At critical point\n   - Convergence achieved\n   - Training can stop\n\nRelationship to step size:\nstep_size = lr × ||∇L||\n\nSo even with constant learning rate, step size decreases naturally as you approach the minimum. This provides automatic annealing.\n\nVisualization:\n- LinearMetricsPanel shows ||∇L|| numerically\n- GradientVectorViz shows it as arrow length\n- Decreasing magnitude over time indicates healthy convergence",
    "level": ["intermediate", "advanced"],
    "formula": "||\\nabla L|| = \\sqrt{(\\frac{\\partial L}{\\partial w_1})^2 + (\\frac{\\partial L}{\\partial w_2})^2}",
    "relatedTerms": ["gradient-vector", "gradient-direction", "step-size"],
    "example": "In lr-optimal: step 0 has ||∇L|| = 72.08 (steep!), step 25 has ||∇L|| = 8.3, step 50 has ||∇L|| = 1.02, step 100 has ||∇L|| = 0.15 (nearly flat). This exponential decrease signals convergence."
  },
  "gradient-direction": {
    "term": "Gradient Direction",
    "brief": "The angle of the gradient vector, showing which direction in parameter space points uphill.",
    "detailed": "Gradient direction is the angle θ of the gradient vector ∇L, computed as θ = atan2(grad_w2, grad_w1). It shows which direction in (w1, w2) space increases loss most steeply. We move in the opposite direction (-θ) to descend. In circular loss surfaces, direction points toward the center. In elliptical surfaces, direction varies more.",
    "comprehensive": "Gradient direction captures where the gradient vector points in parameter space.\n\nDefinition:\nθ = atan2(∂L/∂w2, ∂L/∂w1)\n\nThis gives angle in radians from -π to π:\n- 0°: points right (+w1 direction)\n- 90°: points up (+w2 direction)  \n- 180° or -180°: points left (-w1 direction)\n- -90°: points down (-w2 direction)\n\nInterpretation:\n1. **Circular contours**: Direction points directly toward center\n   - If you're at (0, 0) and optimum is (2, 1.5)\n   - Gradient should point toward upper-right (≈ 37°)\n   - Direction stable throughout training\n\n2. **Elliptical contours**: Direction varies more\n   - Gradient perpendicular to elongated contours\n   - Direction can rotate significantly\n   - Causes zigzag patterns when combined with high lr\n\n3. **Direction changes**: Indicate curved trajectory\n   - Large rotations mean navigating complex terrain\n   - Small rotations mean direct path\n   - Zero rotation = perfectly straight trajectory\n\nUpdate direction:\nWe move opposite to gradient:\nΔw = -lr × ∇L\nSo actual movement is at angle θ + 180°\n\nVisualization:\n- GradientVectorViz shows direction as arrow angle\n- GradientField shows how direction varies across space\n- ParameterSpace2D trajectory shows actual movement direction",
    "level": ["intermediate", "advanced"],
    "formula": "\\theta = \\text{atan2}(\\frac{\\partial L}{\\partial w_2}, \\frac{\\partial L}{\\partial w_1})",
    "relatedTerms": ["gradient-vector", "trajectory", "parameter-space"],
    "example": "In lr-optimal at step 0: gradient points at θ ≈ -136° (lower-left), so we move at ≈ 44° (upper-right toward optimum). At step 50, gradient still points toward origin (now ≈ -148°), showing consistent direction."
  },
  "update-vector": {
    "term": "Update Vector",
    "brief": "The actual change applied to parameters: Δw = -α·∇L. This is the step taken in parameter space.",
    "detailed": "The update vector is -lr × gradient, representing the actual movement in parameter space. It's scaled opposite to the gradient: where gradient points uphill, update points downhill. The learning rate controls the update size. Visualizing update vectors (green in GradientVectorViz) shows the optimization steps actually taken, while gradient vectors (blue) show the steepest direction.",
    "comprehensive": "The update vector translates gradient information into parameter changes.\n\nDefinition:\nΔw = [Δw1, Δw2] = -α · ∇L = -α · [∂L/∂w1, ∂L/∂w2]\n\nComponent-wise:\nΔw1 = -α · ∂L/∂w1\nΔw2 = -α · ∂L/∂w2\n\nParameter update:\nw1_new = w1_old + Δw1\nw2_new = w2_old + Δw2\n\nKey properties:\n1. **Opposite to gradient**: If gradient points up, update points down\n2. **Scaled by lr**: Determines step size\n3. **Direction**: Always points toward lower loss\n4. **Magnitude**: ||Δw|| = α × ||∇L||\n\nVisualization:\n- Green arrow in GradientVectorViz\n- Shorter than gradient by factor of lr\n- Points opposite direction\n- Actual movement shown in ParameterSpace2D\n\nUpdate vs Gradient:\n- Gradient (blue): Tells you which way is uphill\n- Update (green): Shows where you actually move\n- Relationship: Update = -lr × Gradient\n\nStep size:\n||Δw|| = α × ||∇L||\n\nEarly training: large gradient → large step\nLate training: small gradient → small step\nThis automatic annealing aids convergence.",
    "level": ["intermediate", "advanced"],
    "formula": "\\Delta w = -\\alpha \\cdot \\nabla L = -\\alpha \\cdot \\begin{bmatrix} \\frac{\\partial L}{\\partial w_1} \\\\ \\frac{\\partial L}{\\partial w_2} \\end{bmatrix}",
    "relatedTerms": ["gradient-vector", "gradient-magnitude", "step-size"],
    "example": "At step 0 in lr-optimal: ∇L = [-52.0, -49.9], lr = 0.01. Update = -0.01 × [-52.0, -49.9] = [0.52, 0.50]. So w1 increases by 0.52 and w2 increases by 0.50, moving from (0,0) to (0.52, 0.50)."
  },
  "anisotropy": {
    "term": "Anisotropy",
    "brief": "Property of having different characteristics in different directions. Anisotropic loss surfaces are harder to optimize.",
    "detailed": "An anisotropic loss surface has elongated contours (ellipses instead of circles), meaning loss sensitivity differs dramatically across parameter directions. This occurs when input features have mismatched scales. Gradient descent struggles because one learning rate can't suit all directions - what's optimal for one parameter causes overshooting in another.",
    "comprehensive": "Anisotropy (opposite: isotropy) means 'directionally dependent behavior'.\n\nIn optimization:\n\n**Isotropic surface** (lr-optimal):\n- Circular contours\n- Loss sensitivity similar in all directions\n- ∂²L/∂w1² ≈ ∂²L/∂w2² (similar curvatures)\n- Single learning rate works well\n- Direct path to optimum\n- Caused by: features with similar scales\n\n**Anisotropic surface** (anisotropic-hard):\n- Elliptical contours (elongated)\n- Loss very sensitive in some directions, not others\n- ∂²L/∂w1² >> ∂²L/∂w2² (or vice versa)\n- Single learning rate problematic\n- Zigzag path to optimum\n- Caused by: feature scale mismatch\n\nWhy it happens:\n\nFor y = w1*x1 + w2*x2:\n- If x1 ranges 0-1 but x2 ranges 0-20\n- Then loss is 20× more sensitive to w2 than w1\n- Loss surface becomes 20× narrower in w2 direction\n- Gradients: grad_w2 typically 10-20× larger than grad_w1\n\nConsequences:\n- Learning rate too high for steep direction (overshooting)\n- Learning rate too low for gentle direction (slow progress)\n- Compromise: reduce lr → slow convergence with mild zigzagging\n- Better solution: scale features to similar ranges\n\nMeasuring anisotropy:\n- Condition number: ratio of largest to smallest eigenvalue of Hessian\n- Aspect ratio: ratio of ellipse major to minor axis\n- anisotropic-easy: ~10:1, anisotropic-hard: ~20:1\n\nSolutions:\n1. **Feature scaling**: Standardize inputs to mean=0, std=1\n2. **Adaptive optimizers**: Adam, RMSprop (per-parameter learning rates)\n3. **Momentum**: Helps smooth out zigzag oscillations\n4. **Preconditioning**: Transform to circular contours",
    "level": ["intermediate", "advanced"],
    "formula": "\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} \\text{ (condition number)}",
    "relatedTerms": ["contour-plot", "feature-scaling", "zigzag-pattern", "isotropic-surface"],
    "example": "In anisotropic-hard: x1 ranges 0-1, x2 ranges 0-20. At step 10, grad_w1 = -12.3, grad_w2 = -156.8 (13× difference!). With lr=0.008: Δw1 = 0.10 (good), Δw2 = 1.25 (overshoots). Result: trajectory bounces back and forth."
  },
  "isotropic-surface": {
    "term": "Isotropic Surface",
    "brief": "A loss surface where loss changes equally in all directions. Results in circular contours and easy optimization.",
    "detailed": "An isotropic surface has circular contours, meaning loss sensitivity is equal in all directions. This is the ideal scenario for gradient descent: the algorithm can use a single learning rate effectively, paths are direct, and convergence is efficient. Isotropy occurs when all input features have similar scales and the problem is well-conditioned.",
    "comprehensive": "Isotropic means 'same properties in all directions' - the opposite of anisotropic.\n\nCharacteristics:\n1. **Circular contours**: Loss value c forms a circle in parameter space\n2. **Equal curvature**: ∂²L/∂w1² ≈ ∂²L/∂w2²\n3. **Condition number** ≈ 1: well-conditioned problem\n4. **Gradient direction**: Points directly toward optimum\n5. **Trajectory**: Smooth, direct path\n\nWhy it's ideal:\n- Single learning rate works across all directions\n- Can use larger learning rates (faster convergence)\n- No zigzag patterns\n- Predictable behavior\n- Efficient optimization\n\nHow to achieve isotropy:\n1. **Feature scaling**: Most important\n   - Standardization: (x - mean) / std\n   - Min-max scaling: (x - min) / (max - min)\n   - Brings all features to similar ranges\n\n2. **Whitening**: Advanced preprocessing\n   - Decorrelate features\n   - Equal variance in all directions\n   - Uses PCA or ZCA\n\n3. **Natural isotropy**: Some problems are naturally isotropic\n   - Features already similar scale\n   - Balanced contributions to loss\n\nExample cases:\n- lr-optimal: Nearly perfect isotropy (circular contours)\n- lr-small/large: Same isotropic surface, different learning rates\n- Compare with anisotropic-hard to see the difference\n\nMathematically:\nFor perfect isotropy:\nL(w1, w2) = c · (w1² + w2²)\nGradient always points radially toward origin\nContours are perfect circles",
    "level": ["intermediate", "advanced"],
    "formula": "L(w_1, w_2) = c \\cdot (w_1^2 + w_2^2) \\text{ (perfect isotropy)}",
    "relatedTerms": ["anisotropy", "contour-plot", "feature-scaling", "loss-surface"],
    "example": "In lr-optimal: contours centered at (2.0, 1.5) are nearly circular with radius proportional to loss. At distance 1.0 from center, loss ≈ 2.5 in all directions (north, south, east, west equally). This symmetry enables efficient optimization."
  },
  "zigzag-pattern": {
    "term": "Zigzag Pattern",
    "brief": "An oscillating trajectory where optimization bounces back and forth across the valley instead of descending smoothly.",
    "detailed": "A zigzag pattern occurs when the learning rate is too large relative to the loss surface curvature, causing parameter updates to overshoot the valley bottom and bounce to the opposite side. Each step jumps across the valley, creating a saw-tooth path. This is inefficient but often still converges. Common in high learning rate scenarios or anisotropic surfaces.",
    "comprehensive": "Zigzag patterns are the signature of overshooting in gradient descent.\n\nCauses:\n\n1. **Learning rate too large**:\n   - Step size > valley width\n   - Gradient points across valley\n   - Update jumps to opposite side\n   - Next gradient points back\n   - Result: bouncing back and forth\n   - See: lr-large case\n\n2. **Anisotropic surface**:\n   - Even with tuned lr, one direction overshoots\n   - Steep direction gets large gradient\n   - Update is too big for that direction\n   - Result: zigzag along narrow valley\n   - See: anisotropic-hard case\n\n3. **Combined effect** (worst case):\n   - High lr + anisotropic surface\n   - Dramatic oscillations\n   - See: zigzag-convergence case\n\nMechanism:\n1. Start at position A on left valley wall\n2. Gradient points right-down (toward valley bottom and forward)\n3. Large update jumps past valley bottom to position B on right wall\n4. Gradient now points left-down (opposite horizontal, same forward)\n5. Update jumps back to left wall at position C (further forward)\n6. Repeat: positions alternate left-right while moving forward\n\nConsequences:\n- Wastes computation (indirect path)\n- Takes more steps to converge\n- But typically still converges (for convex problems)\n- In neural networks: may fail to converge or find worse solution\n\nSolutions:\n1. **Reduce learning rate**: Most direct fix\n2. **Feature scaling**: If caused by anisotropy\n3. **Momentum**: Smooths out oscillations\n4. **Adaptive optimizers**: Adjust per-parameter learning rates\n\nHow to diagnose:\n- ParameterSpace2D shows characteristic zigzag\n- LossContour shows trajectory crossing valley repeatedly\n- Loss may oscillate slightly while generally decreasing",
    "level": ["beginner", "intermediate", "advanced"],
    "formula": "\\text{If } \\alpha \\cdot ||\\nabla L|| > \\text{valley width, then zigzag occurs}",
    "relatedTerms": ["anisotropy", "trajectory", "update-vector"],
    "example": "In lr-large: at step 5, w=(0.9, 0.8) on left. Gradient points right. Update jumps to (1.8, 1.6) on right. At step 6, gradient points left. Update jumps back to (2.1, 1.9) on left. Path oscillates left-right while progressing toward (2.0, 1.5)."
  },
  "feature-scaling": {
    "term": "Feature Scaling",
    "brief": "Normalizing input features to similar ranges (e.g., mean=0, std=1) to create isotropic loss surfaces.",
    "detailed": "Feature scaling transforms input variables to similar numerical ranges, typically standardization (mean 0, std 1) or min-max scaling (0 to 1). This creates more isotropic loss surfaces with circular contours, enabling larger learning rates and faster, smoother convergence. It's one of the most important preprocessing steps in machine learning.",
    "comprehensive": "Feature scaling addresses the problem demonstrated in anisotropic cases.\n\nCommon methods:\n\n1. **Standardization** (Z-score normalization):\n   x_scaled = (x - mean) / std\n   - Centers at zero\n   - Standard deviation = 1\n   - Preserves outliers\n   - Most common in ML\n\n2. **Min-Max scaling**:\n   x_scaled = (x - min) / (max - min)\n   - Range: [0, 1]\n   - Sensitive to outliers\n   - Good for neural networks\n\n3. **Robust scaling**:\n   x_scaled = (x - median) / IQR\n   - Resistant to outliers\n   - Uses interquartile range\n\nWhy it matters:\n\nWithout scaling:\n- Feature 1: [0, 1] → small x1 values\n- Feature 2: [0, 20] → large x2 values\n- Loss: sensitive to w2, insensitive to w1\n- Result: elongated contours, zigzag optimization\n\nWith scaling:\n- Feature 1: [0, 1] (already good)\n- Feature 2: [0, 20] → [0, 1] (scaled down)\n- Loss: equal sensitivity to w1 and w2\n- Result: circular contours, smooth optimization\n\nImpact on gradient descent:\n- Before: grad_w1 ≈ 12, grad_w2 ≈ 156 (13× difference)\n- After: grad_w1 ≈ 45, grad_w2 ≈ 52 (similar)\n- Learning rate can be 10× larger\n- Converges 5-10× faster\n\nImplementation (Python):\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nWhen to use:\n- Always for gradient descent algorithms\n- Always for neural networks\n- Always when features have different units\n- Not needed for tree-based models (random forest, XGBoost)",
    "level": ["intermediate", "advanced"],
    "formula": "x_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma}",
    "relatedTerms": ["anisotropy", "isotropic-surface", "contour-plot"],
    "example": "anisotropic-hard before scaling: x1:[0,1], x2:[0,20] → elliptical contours, slow convergence. After scaling both to [0,1] → circular contours, 5× faster convergence. Compare lr-optimal (scaled) vs anisotropic-hard (unscaled) to see the difference."
  },
  "gradient-field": {
    "term": "Gradient Field",
    "brief": "A vector field showing the gradient vector -∇L at every point in parameter space, revealing the 'flow' of optimization.",
    "detailed": "The gradient field visualizes descent directions across the entire parameter space, not just along your trajectory. Each arrow shows which way gradient descent would move if starting from that position. Arrow color indicates gradient magnitude (blue=small, red=large). This reveals the global structure of optimization, showing that all paths lead toward the minimum.",
    "comprehensive": "The gradient field is a vector field defined over parameter space.\n\nMathematical definition:\nF(w1, w2) = -∇L(w1, w2) = -[∂L/∂w1, ∂L/∂w2]\n\nAt each point (w1, w2), there's an arrow pointing in the descent direction.\n\nVisualization:\n- Computed on a grid (e.g., 10×10 or 20×20)\n- Each grid point gets an arrow\n- Arrow direction: -∇L (descent direction)\n- Arrow color: ||∇L|| (magnitude)\n- Arrow length: optionally scaled by magnitude\n\nKey insights:\n\n1. **Flow toward optimum**: All arrows point generally toward minimum\n   - Like water flowing downhill\n   - Multiple starting points → same destination\n   - Reveals basin of attraction\n\n2. **Magnitude patterns**: \n   - Large arrows (red) far from minimum\n   - Small arrows (blue) near minimum\n   - Very small at center (critical point)\n\n3. **Perpendicular to contours**: \n   - Arrows always perpendicular to contour lines\n   - Mathematical property of gradients\n   - Steepest descent is perpendicular to level curves\n\n4. **Predicts trajectories**:\n   - Your trajectory follows arrows\n   - Can predict path from any starting point\n   - Explains why certain paths emerge\n\nDensity options:\n- Coarse (10×10): Fewer arrows, cleaner view\n- Fine (20×20): More detail, denser visualization\n\nRelation to other visualizations:\n- GradientVectorViz: Shows one arrow (current position)\n- GradientField: Shows many arrows (entire space)\n- Both reveal descent direction structure",
    "level": ["intermediate", "advanced"],
    "formula": "\\mathbf{F}(w_1, w_2) = -\\nabla L(w_1, w_2)",
    "relatedTerms": ["gradient-vector", "parameter-space", "loss-surface"],
    "example": "In lr-optimal gradient field: at (0, 0) arrow points toward (2.0, 1.5) with red color (magnitude ~72). At (1.0, 1.0) arrow still points toward center with orange color (magnitude ~25). At (2.0, 1.5) arrow is nearly zero (blue, at minimum). All arrows converge on the center."
  },
  "heatmap": {
    "term": "Heatmap",
    "brief": "A color-coded visualization where color represents a value. In Phase 2, color shows loss at each (w1, w2) position.",
    "detailed": "A heatmap uses color to represent numerical values across a 2D space. In the LossContour visualization, each pixel's color represents the loss value at that (w1, w2) coordinate. Blue indicates low loss (good regions), red indicates high loss (bad regions), and yellow/orange are intermediate. This creates an intuitive 'hot' (avoid) vs 'cold' (seek) visualization of the loss surface.",
    "comprehensive": "Heatmaps provide intuitive visualization of 2D functions through color.\n\nIn Phase 2:\n- X-axis: w1 values\n- Y-axis: w2 values\n- Color: L(w1, w2) loss value\n\nColor scheme:\n- Blue: Low loss (L < 5) - goal region\n- Cyan: Moderate loss (5 < L < 20)\n- Yellow: Medium loss (20 < L < 50)\n- Orange: High loss (50 < L < 80)\n- Red: Very high loss (L > 80) - avoid\n\nAdvantages:\n1. **Intuitive**: Color naturally communicates good/bad\n2. **Dense information**: Every pixel has meaning\n3. **Pattern recognition**: Human vision excels at color patterns\n4. **Gradient visibility**: Color transitions show slope\n\nLimitations:\n1. **Discrete**: Computed on finite grid (50×50 in our case)\n2. **Color perception**: Varies across people/displays\n3. **Numerical precision**: Hard to read exact values\n\nImplementation:\n- Pre-compute loss on 50×50 grid\n- Normalize loss to [0, 1]\n- Map to color scale (blue → cyan → yellow → red)\n- Render to canvas\n- Overlay trajectory and markers (SVG)\n\nInteraction with trajectory:\n- Trajectory should move from red→yellow→cyan→blue\n- Current position shows local loss via background color\n- Reveals why optimizer made specific moves\n\nAlternatives:\n- Contour lines: Show discrete levels\n- 3D surface: Show true height (not used here)\n- Combined: Heatmap + contour lines (best of both)",
    "level": ["beginner", "intermediate"],
    "formula": "\\text{color}(w_1, w_2) = \\text{colormap}(L(w_1, w_2))",
    "relatedTerms": ["loss-surface", "contour-plot", "gradient-field"],
    "example": "In LossContour for lr-optimal: origin (0,0) shows red (loss=89), point (1,1) shows yellow (loss≈10), point (1.9,1.4) shows cyan (loss≈2), center (2.0,1.5) shows deep blue (loss≈0). The gradient from red to blue guides your eye toward the optimum."
  },
  "step-size": {
    "term": "Step Size",
    "brief": "The distance traveled in parameter space during one update: ||Δw|| = lr × ||∇L||.",
    "detailed": "Step size is the magnitude of the parameter update vector, determining how far parameters move in one iteration. It equals learning rate times gradient magnitude. Early in training with steep gradients, steps are large. Near convergence with flat gradients, steps become tiny. This automatic decrease aids convergence even with fixed learning rate.",
    "comprehensive": "Step size measures the distance of parameter movement per iteration.\n\nDefinition:\nstep_size = ||Δw|| = ||−α · ∇L|| = α · ||∇L||\n\nComponents:\n- α: learning rate (fixed hyperparameter)\n- ||∇L||: gradient magnitude (changes each step)\n- Product determines actual movement distance\n\nDynamic behavior:\n\nStep 0 (far from minimum):\n- ||∇L|| ≈ 72 (steep slope)\n- step_size = 0.01 × 72 = 0.72 (large step)\n- Parameters change significantly\n\nStep 50 (moderate distance):\n- ||∇L|| ≈ 8 (moderate slope)\n- step_size = 0.01 × 8 = 0.08 (medium step)\n- Steady progress\n\nStep 100 (near minimum):\n- ||∇L|| ≈ 0.5 (gentle slope)\n- step_size = 0.01 × 0.5 = 0.005 (tiny step)\n- Fine-tuning\n\nKey insight: Automatic annealing\n- Even with constant lr, step size decreases\n- Natural slowing near minimum\n- Helps convergence without lr schedule\n- Convex problems: ||∇L|| → 0 at minimum\n\nTradeoffs:\n\nToo large step size:\n- Overshoots minimum\n- Zigzag patterns\n- Slow or no convergence\n- Solution: reduce lr\n\nToo small step size:\n- Slow progress\n- Many iterations needed\n- Wastes computation\n- Solution: increase lr\n\nOptimal step size:\n- Makes good progress\n- No overshooting\n- Converges efficiently\n- Problem-dependent\n\nVisualization:\n- LinearMetricsPanel displays step_size\n- ParameterSpace2D shows as line segment length\n- Decreasing step_size over time indicates healthy convergence",
    "level": ["intermediate", "advanced"],
    "formula": "\\text{step_size} = ||\\Delta w|| = \\alpha \\cdot ||\\nabla L||",
    "relatedTerms": ["gradient-magnitude", "update-vector", "trajectory"],
    "example": "lr-optimal with lr=0.01: step 0 has ||∇L||=72, step_size=0.72 (moves 0.72 units in parameter space). Step 100 has ||∇L||=0.5, step_size=0.005 (moves only 0.005 units). This 144× reduction in step size happens automatically!"
  }
}
